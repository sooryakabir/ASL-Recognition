# American Sign Language Recognition with Convolutional Neural Networks

## Project Description

This project focuses on the use of deep learning techniques, specifically Convolutional Neural Networks (CNNs), to recognize and classify American Sign Language (ASL) gestures. The goal is to develop a robust system capable of interpreting ASL signs and translating them into text or speech, thereby bridging communication gaps for the deaf and hard-of-hearing communities.

## Abstract

The integration of AI, deep learning, and machine learning has revolutionized service technology, particularly in telecommunications, enhancing language discovery and comprehension. This project explores AI's potential to bridge communication divides by developing a system that interprets ASL signs and translates them into text or speech. Utilizing a CNN model, the project demonstrates the effectiveness of deep learning in recognizing and translating abstract expressions found in sign language.

## Installation

To run this project, you'll need to have Python and the necessary packages installed. 

## Model Accuracies

| Model | Accuracy |
|-------|----------|
| CNN   | 95.67%   |
| SVC   | 97.12%   |
| KNN   | 86.54%   |

These results indicate the effectiveness of each model in recognizing American Sign Language signs, with the Support Vector Classifier (SVC) achieving the highest accuracy.


## Authors

- [Raghav Sampath](https://github.com/thehyperpineapple)
- [Aditya Verma](https://github.com/TheLetifer)
- [Soorya P Kabir](https://github.com/sooryakabir)
- [Archit Patro](https://github.com/Archit-Patro)
- [N Aaron Abhishay](https://github.com/aaronabhishay)

## Institution

School of Electronics Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu, India.
